
# Eighth & Ninth Exercises :pencil:
The eighth and ninth exercises in this course are about splitting the workflow to implement CPU parallelization. Specifically, the *daxpy* code is revisited as an example and instead of using a single *for loop* to add all the elements of the vectors, the work is split into **chunks**. A chunk is a set of data which is a subset of the workload or data assigned to a thread or processor. Splitting the code into chunks is therefore well-suited for CPU parallelization. In this exercise, this is achieved using two frameworks: *OpenMP* and *OpenMPI*. All the files can be found [here](https://github.com/EnricoSc1/SCPS/tree/main/code/Eighth_Ninth_Exercise).

## OpenMP :one:
[Multi-threading](https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)) is a technique where a process is divided into smaller execution units, called *threads*, that run concurrently. [*OpenMP*](it.wikipedia.org/wiki/OpenMP) uses a multi-threading approach, where a process is divided into smaller execution units called *threads* that run concurrently. Here the *OpenMP* directives are used to execute the *daxpy* sum in parallel. The headers and initializations are given by   
<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_init.png" />
</center>

The main *for loop* cycles over the dimensions of the array `N[]` while the nested loops represent the implementation of the splitting. The outer loop cycles through the chunks, and the inner loop cycles through their elements. In particular, the final chunk has been properly fixed to account for the possible memory excess due to the division. Multi-thread parallelization is implemented via the comment line
>#pragma omp parallel for private(thread_id) reduction(&&:local_check) reduction(+:sum)

that tells the compiler to parallelize the *for loop* using multiple threads. Each thread gets its own independent copy of *thread_id* so as not to affect the other copies. `reduction( operation : object )` combines, through *operation*, the *objects* for all the threads at the end of the parallel region. In this case the reductions return the total sum and the final check. At the end of the code, the dynamically allocated memory for the `x`, `y` and  `d` vectors is released and a timer is set to print out the execution time.
<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_MP.png"/>
</center>
<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_MP_end.png"/>
</center>

## OpenMPI :two:
MPI directives work differently to OpenMP directives. Parallelization provides a copy of the code to each processing unit, each of which is labelled with a unique rank. The code is then executed simultaneously by all processors, with each chunk being computed on a different processor according to the ranking variable, which enters explicitly into the computation. 
<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_MPI_head.png" />
</center>

Here the three lines of code

>MPI_Init(NULL, NULL);
>MPI_Comm_size(MPI_COMM_WORLD, &numprocs);
>MPI_Comm_rank(MPI_COMM_WORLD, &rankp);

initialize the MPI parallelization, together with the rank and the total number of processing units. The separation in chunks is similar to the previous one: inside the *for loop* are initialized `start`, `end` and `chunk_size` using explicitly the variables `numprocs` and `rankp`. In this way each copy of the code is executed differently by each processing unit. The *daxpy sum* is unaltered, however the results of every operation are summed together by [MPI_Reduce()](https://www.mpich.org/static/docs/v4.1/www3/MPI_Reduce.html) as
 >MPI_Reduce(&sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

that performs a global reduction operation (sum) on all members, storing the final result in `global_sum`. One processing unit is then picked to print out the result. If *printf()* is used inside the parallel region, every process participating in the computation will print its own copy of the message to the terminal in a duplicated or cluttered output. To avoid this, the **rank 0** is selected in a conditional statement to handle tasks managing the input or output, acting as a *main* processor that gathers results from other ranks. The parallelization environment is terminated by `MPI_Finalize()` and the run time is printed out. 
 
<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_MPI_for.png" />
</center>

<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_MPI_end.png" />
</center>

## Final output
Here is the final output. Both parallelizations improve computation time compared to the serial execution, moreover the processor-based (OpenMPI) parallelization is slightly faster than the multi-threaded (OpenMP) one.
<center>
<img src="https://raw.githubusercontent.com/EnricoSc1/SCPS/refs/heads/main/images/ex_89_output.png"/>
</center>



